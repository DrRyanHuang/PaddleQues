### 20230901 11:00

#### 1. 新 IR 之后, 仅仅开始走静态图分支? 与动态图关系不大嘛?

![https://user-images.githubusercontent.com/23097963/264596070-96eeba57-2b3e-4013-80dc-694a37cf6246.png](https://user-images.githubusercontent.com/23097963/264596070-96eeba57-2b3e-4013-80dc-694a37cf6246.png)

是的，动态图直接走 Kernel，之后会有旧 IR 的退场，我们可以先关闭掉 Python 的 API, 之后再逐步封闭掉



#### 2. GradOp 转译 （`MulGradOpTranscriber`） 主要就是一些 有效性校验 和 将 `X@GRAD` 和 `Y@GRAD` 添加到 `param_map` 就可以了嘛？ 

`param_map` 主要存放了输入 Tensor 和 每个参数的 grad , 还有存放了别的数据吗?

```c++
    const auto& y_grad_output = op_desc.Output("Y@GRAD");
    if (y_grad_output.size() < 1) {
      return;
    }
    IR_ENFORCE(
        y_grad_output.size() == 1,
        "Expected op[%s]'s output Y@GRAD has only 1 variable, but got %d",
        op_desc.Type(),
        y_grad_output.size());
    const auto& y_grad_var_name = y_grad_output[0];

    auto idx_iter_y = arg_to_idx.find(y_grad_var_name);
    if (idx_iter_y == arg_to_idx.end()) {
      IR_THROW("op[%s] should have got its y_grad", op_desc.Type());
    }
    auto [idx_in_op_y, idx_in_vec_y] = idx_iter_y->second;
    VLOG(10) << "[output recording]"
             << "[" << op_desc.Type() << "]" << y_grad_var_name << " "
             << idx_in_op_y << " " << idx_in_vec_y;

    auto y_names = op_desc.Input("Y", true);
    auto y_name = y_names[0];
    IR_ENFORCE(param_map->count(y_name) > 0,
               "Expected op[%s]'s input %s has been parsed",
               op_desc.Type(),
               y_name);
    auto y_defining_info = param_map->at(y_name);
    ir::OpResult y_value = y_defining_info.value;
    IR_ENFORCE(y_value,
               "Expected op[%s]'s input %s is not null",
               op_desc.Type(),
               y_name);
    ir::Type y_type = y_value.type();
    IR_ENFORCE(y_type.isa<dialect::DenseTensorType>(),
               "Expected op[%s]'s input %s is DenseTensor but got %s",
               op_desc.Type(),
               y_name,
               y_type);

    dialect::DenseTensorType y_tensor_type =
        y_type.dyn_cast<dialect::DenseTensorType>();
    std::vector<int64_t> y_shape = phi::vectorize(y_tensor_type.dims());

    ir::OpResult y_value_res = operation->result(idx_in_op_y);
    auto reshape_op_y = builder.Build<dialect::ReshapeOp>(y_value_res, y_shape);
    (*param_map)[y_grad_var_name] =
        VariableDefiningInfo(reshape_op_y.out(), false, -1);
```

前反向也需要 `reshape` 操作, 注意 `reshape` 的插入时机, 要和正向的相对应



#### 3. 目前了解了用法 `builder.Build<dialect::ReshapeOp>`, 想看一下 dialect namespace 下的所有 Op, 是与 `paddle/fluid/operators` 下的Op对应的嘛? 还是有其他位置存放，想问下有没有统一的 C++ Op 文档呢?

如何与 Python API 对应
- `paddle/fluid/pybind/static_op_function.cc`
- `paddle/fluid/ir/dialect/paddle_dialect/it/pd_api.h`

比如这个函数:
```
PyObject *static_api_abs(PyObject *self, PyObject *args, PyObject *kwargs);
```


#### 4. 有些文件或者变量，都有后缀 `Impl`，比如变量 `OpLowerer` 与 `OpLowererImpl`，文件 `op_lowering.h` 与 `op_lowering_impl.h`，这是一些命名规范吗? 或者在实现上有什么通用的规则吗？

- 文件用`_`, 变量是驼峰，`namespace` 是小写
- 用于编译时封装的 Pimpl, pimpl idiom 是一种新式 C++ 技术，用于隐藏实现、最小化耦合和分离接口。 Pimpl 是“指向实现的指针”的缩写。


#### 5. 杰哥可以再给一个新任务吗，[56550](https://github.com/PaddlePaddle/Paddle/pull/56550) 完成的差不多了，我浅浅的了解了新IR，想再来几个小任务实践一下